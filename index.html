<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TSCnet:A Text-driven Semantic-level Controllable Module for Personalized Low-Light Image Enhancement</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MDANet: a Multi-Stage Domain Adaptation Framework for Generalizable Low-Light Image Enhancement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">  
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jianhui Wang</a><sup>†</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yangfan He</a><sup>†</sup>,</span>
                  <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Pengyu Zeng</a>,</span>
                  <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Li Kun</a>,</span>
                  <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Sida Li</a>,</span>
                  <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lan Zhao</a>,</span>
                  <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jun Yin</a>,</span>
                  <span class="author-block">
                                      <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Miao Zhang</a><sup>*</sup>
                  <span class="author-block">
                                      <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Tianyu Shi</a>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Shenzhen International Graduate School
Tsinghua University</span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>


            
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Low-light image enhancement is a vital process that seeks to enhance the visibility and clarity of images taken under suboptimal lighting conditions. Traditional methods for low-light enhancement rely on altering illumination levels but struggle with local variations in lighting levels and complex scenes, often producing artifacts, color distortions, or unnatural appearances. Recent deep learning-based methods have improved performance through robust feature extraction but usually rely on single-domain datasets that cannot generalize to all lighting conditions present in real-life situations. To address these generalization challenges, we propose MDANet: a Multi-Stage Domain Adaptation framework that sequentially implements light, focus, and contextual domain adaptation. First, MDANet employs an Adaptive Lighting Enhancement Module that combines an innovative U-Net architecture with cross-domain contrastive learning for lighting invariance. Second, this framework utilizes large language models (LLMs) for context-aware adjustments while incorporating depth map constraints within an innovative Region-Focused Enhancement Module that selectively targets and processes key focal regions. Third, a Contextual Adaptation Module employs multitask meta-learning to ensure robustness and superior performance across a range of contextual scenarios.  Experimental results demonstrate that MDANet consistently outperforms both traditional and state-of-the-art methods by offering both improved image quality as well as generalization across diverse low-light scenarios. \textcolor{blue}{Experimental results show that MDANet achieves state-of-the-art performance across multiple datasets, including MUSIQ scores of 38.219 on LOL, 39.675 on SID, and 39.825 on RELLISUR, outperforming all compared methods in both perceptual and quantitative metrics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



  
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="head.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">The overview of low light enhance tasks.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="framework_all.png" lt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">This framework introduces a multi-stage domain adaptation approach for low-light image enhancement. It incorporates an Adaptive Lighting Enhancement Module (top) that learns light invariance through cross-domain contrastive learning, a Region-Focused Enhancement Module (middle) that leverages large language model prompts for high-level semantic information and depth images for spatial domain information to achieve selective key area improvement, and a Contextual Adaptation Module (bottom) that fine-tunes model responses to various lighting conditions using meta-learning and multi-scale LoRA layer strategies. Each module collaborates sequentially to achieve light, focus, and contextual domain adaptation, resulting in optimal visual enhancement and strong generalization capabilities.
        </h2>
      </div>
     <div class="item">
      <!-- Your image here -->
      <img src="MC-Adapter.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">The MC-adapter reshapes and processes input features in parallel, combining them across branches while retaining original dimensions for refined output.
      </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carouse2 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="comparison.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">Visual comparison of different low-light image enhancement models using Exdark dataset.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="lol1.png" lt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">Visual comparison with other state-of-the-art methods on LOL1.
        </h2>
      </div>
     <div class="item">
      <!-- Your image here -->
      <img src="lol2.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">Visual comparison with other advanced approaches on LOM and LOL2.
      </h2>
     </div>
      <div class="item">
      <!-- Your image here -->
      <img src="openworld.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">Visual comparison with other advanced approaches on DCIM, REAL, LIME and MEF.
      </h2>
     </div>
      <div class="item">
      <!-- Your image here -->
      <img src="low_light_centric.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">Visual comparison with other state-of-the-art methods on MEF.
      </h2>
     </div>
      <div class="item">
      <!-- Your image here -->
      <img src="REAL.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">Visual comparison with other state-of-the-art methods on REAL.
      </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

  
<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="local_enhance.png" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
          Results of different tasks, Task A and B aimed at decreasing it and some others aimed at increasing brightness, covering a range of scenes such
          as stage performances, everyday environments, and medical images. The tasks, labeled A through E, involve modifying the brightness of a masked object
          or area (main character, lady, blackboard, side representing evil, left lung) by a percentage ranging from 10% to 40%.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="Flexible_Adjustment.png" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
         The application of natural language processing enables complex lighting adjustments in images. 
          In each task, natural language instructions are used to brighten both target and background lighting adjustments, 
          all driven by linguistic input. 
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Single image container -->
      <div class="item has-text-centered">
        <img src="PR_darkface_all.png" alt="MY ALT TEXT" />
        <!-- Subtitle text container with centered text -->
        <p class="subtitle">
Face detection performance in low-light conditions is shown across different methods. The figure  presents visual comparisons of face detection, using various enhancement techniques combined with DSFD. The "Ours + DSFD" delivers the clearest and most accurate results compared to raw input, EnlightenGAN, KinD++, LLflow, and ZeroDCE.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
